# dvsmart_indexing_api

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n General](#-descripciÃ³n-general)
- [Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
- [Arquitectura del Sistema](#-arquitectura-del-sistema)
  - [Diagrama de Componentes](#diagrama-de-componentes)
  - [Flujo de Procesamiento](#flujo-de-procesamiento)
  - [Arquitectura Hexagonal](#arquitectura-hexagonal)
  - [TÃ©cnicas de Procesamiento](#tÃ©cnicas-de-procesamiento)
  - [Sistema de AuditorÃ­a](#sistema-de-auditorÃ­a)
- [Requisitos Previos](#-requisitos-previos)
- [InstalaciÃ³n y Setup](#-instalaciÃ³n-y-setup)
- [GuÃ­a Completa de ConfiguraciÃ³n](#-guÃ­a-completa-de-configuraciÃ³n)
- [ConfiguraciÃ³n de Alto Rendimiento](#-configuraciÃ³n-de-alto-rendimiento)
- [Uso y API](#-uso-y-api)
- [MonitorizaciÃ³n y Observabilidad](#-monitorizaciÃ³n-y-observabilidad)
- [Troubleshooting](#-troubleshooting)
- [Mantenimiento y Testing](#-mantenimiento-y-testing)
- [Referencias](#-referencias)
- [Soporte y Contacto](#-soporte-y-contacto)

---

## ğŸ¯ DescripciÃ³n General

**dvsmart_indexing_api** es un microservicio empresarial de alto rendimiento diseÃ±ado para indexar masivamente archivos almacenados en servidores SFTP. El sistema procesa millones de archivos de forma distribuida, extrayendo metadata y persistiÃ©ndola en MongoDB para su posterior consulta y organizaciÃ³n.

### Casos de Uso Principales

- **IndexaciÃ³n masiva**: Procesamiento de 11M+ archivos en ~30-60 minutos
- **Descubrimiento de estructura**: Mapeo recursivo de jerarquÃ­as de directorios complejas
- **AuditorÃ­a completa**: Registro detallado de cada ejecuciÃ³n en MongoDB
- **Alta disponibilidad**: Pool de conexiones SFTP con gestiÃ³n automÃ¡tica de recursos

### CaracterÃ­sticas Clave

âœ… **Pool de Conexiones SFTP Lazy**: Conexiones creadas bajo demanda, liberadas automÃ¡ticamente  
âœ… **Procesamiento AsÃ­ncrono**: Pipeline de 3 etapas (Reader â†’ Processor â†’ Writer) con paralelismo configurable  
âœ… **Bulk Upserts a MongoDB**: 3000-5000 documentos/segundo vs 100-200 con operaciones individuales  
âœ… **Persistencia Dual**: PostgreSQL para metadatos de Spring Batch, MongoDB para archivos indexados y auditorÃ­a  
âœ… **Sistema de AuditorÃ­a**: Registro completo de mÃ©tricas, throughput y estado de cada ejecuciÃ³n  
âœ… **MonitorizaciÃ³n Integrada**: Actuator + endpoints custom para observabilidad del pool SFTP y jobs  
âœ… **Resiliente**: ValidaciÃ³n de conexiones, eviction de idle sessions, retry logic  

---

## ğŸ›  Stack TecnolÃ³gico

| TecnologÃ­a | VersiÃ³n | PropÃ³sito |
|------------|---------|-----------|
| **Java** | 21 | Lenguaje base con soporte LTS |
| **Spring Boot** | 4.0.1 | Framework de aplicaciÃ³n |
| **Spring Batch** | 6.0.0 | Procesamiento por lotes de alto rendimiento |
| **Spring Integration** | 7.0.0 | IntegraciÃ³n con sistemas externos (SFTP) |
| **MongoDB** | 5.0+ | Persistencia de metadata de archivos y auditorÃ­a |
| **PostgreSQL** | 12+ | Metadata de Spring Batch (job repository) |
| **Apache Commons Pool2** | - | GestiÃ³n del pool de conexiones SFTP |
| **SSHJ** | 0.38.0 | Cliente SFTP nativo |
| **Lombok** | 1.18.30 | ReducciÃ³n de boilerplate |
| **Maven** | 3.8+ | GestiÃ³n de dependencias y build |

---

## ğŸ— Arquitectura del Sistema

### Diagrama de Componentes

```mermaid
graph TB
    subgraph "Cliente"
        API[REST Controller]
    end

    subgraph "Capa de AplicaciÃ³n"
        UC[StartIndexFullUseCase]
        SVC[StartIndexFullService]
    end

    subgraph "Spring Batch Job"
        JOB[BatchIndexFullJob]
        LISTENER[JobExecutionAuditListener]
        STEP[IndexingStep]
        
        subgraph "Pipeline AsÃ­ncrono"
            READER[DirectoryQueueItemReader]
            PROCESSOR[MetadataExtractorProcessor]
            WRITER[BulkUpsertMongoItemWriter]
        end
    end

    subgraph "Infraestructura"
        POOL[CustomLazySftpSessionFactory]
        MONITOR[SftpPoolMonitor]
        AUDIT_SVC[JobAuditService]
    end

    subgraph "Sistemas Externos"
        SFTP[Servidor SFTP Origen]
        MONGO[(MongoDB)]
        POSTGRES[(PostgreSQL)]
    end

    API -->|POST /api/batch/index/full| UC
    UC --> SVC
    SVC -->|Lanza job| JOB
    JOB -->|beforeJob/afterJob| LISTENER
    LISTENER --> AUDIT_SVC
    JOB --> STEP
    STEP --> READER
    READER -->|Lee archivos| PROCESSOR
    PROCESSOR -->|Extrae metadata| WRITER
    WRITER -->|Bulk upsert| MONGO

    READER -.->|Adquiere sesiÃ³n| POOL
    POOL -.->|ConexiÃ³n SSH| SFTP
    MONITOR -.->|Monitorea| POOL
    JOB -.->|Persiste estado| POSTGRES
    AUDIT_SVC -.->|Guarda auditorÃ­a| MONGO
```

### Flujo de Procesamiento

```mermaid
sequenceDiagram
    participant Client
    participant Controller
    participant AuditListener
    participant BatchJob
    participant Reader
    participant SftpPool
    participant Processor
    participant Writer
    participant MongoDB
    participant PostgreSQL

    Client->>Controller: POST /api/batch/index/full
    Controller->>BatchJob: Lanzar job
    
    Note over BatchJob: JobExecutionListener.beforeJob()
    BatchJob->>AuditListener: beforeJob(jobExecution)
    AuditListener->>MongoDB: INSERT audit record (STARTED)
    BatchJob->>PostgreSQL: Registrar ejecuciÃ³n
    
    Note over BatchJob: Phase 1: Discovery
    BatchJob->>Reader: Descubrir directorios
    Reader->>SftpPool: Obtener sesiÃ³n (1 vez)
    SftpPool-->>Reader: Queue de directorios
    
    Note over BatchJob: Phase 2: Indexing
    loop Por cada chunk de 100 archivos
        Reader->>SftpPool: Obtener sesiÃ³n
        SftpPool-->>Reader: Archivos del directorio
        
        par Procesamiento paralelo (20 threads)
            Reader->>Processor: SftpFileEntry[]
            Processor->>Processor: Extraer metadata
        end
        
        par Escritura paralela
            Processor->>Writer: ArchivoMetadata[]
            Writer->>MongoDB: Bulk upsert (100 docs)
        end
        
        Writer->>PostgreSQL: Commit chunk
    end
    
    Note over BatchJob: JobExecutionListener.afterJob()
    BatchJob->>AuditListener: afterJob(jobExecution)
    AuditListener->>PostgreSQL: Leer mÃ©tricas finales
    AuditListener->>MongoDB: UPDATE audit record (COMPLETED)
    
    BatchJob->>PostgreSQL: Actualizar estado COMPLETED
    BatchJob-->>Controller: JobExecutionId
    Controller-->>Client: 202 Accepted
```

### Arquitectura Hexagonal

```mermaid
graph LR
    subgraph "Dominio"
        M1[ArchivoMetadata]
        M2[JobExecutionAudit]
        S1[FileMetadataService]
        S2[DirectoryDiscoveryService]
        S3[JobAuditService]
    end

    subgraph "Puertos Entrada"
        PI[StartIndexFullUseCase]
    end

    subgraph "Adaptadores Entrada"
        AI1[BatchIndexingController]
        AI2[MonitoringController]
        AI3[JobAuditController]
    end

    subgraph "Adaptadores Salida"
        AO1[DirectoryQueueItemReader]
        AO2[MetadataExtractorProcessor]
        AO3[BulkUpsertMongoItemWriter]
        AO4[CustomLazySftpSessionFactory]
        AO5[JobExecutionAuditListener]
    end

    AI1 --> PI
    AI2 --> PI
    AI3 --> S3
    PI --> S1
    PI --> S2
    S1 --> M1
    S3 --> M2
    AO1 --> S2
    AO1 --> AO4
    AO2 --> S1
    AO3 --> MongoDB[(MongoDB)]
    AO5 --> S3
    AO4 --> SFTP[SFTP Server]
```

### TÃ©cnicas de Procesamiento

#### 1. **Lazy Directory Discovery**

```mermaid
flowchart TD
    A[Primera llamada a read] --> B{Discovery completado?}
    B -->|No| C[Ejecutar BFS en SFTP]
    C --> D[Crear queue de directorios]
    D --> E[Marcar discovery completado]
    E --> F[Retornar primer archivo]
    B -->|SÃ­| G[Retornar archivo de queue actual]
    G --> H{Queue actual vacÃ­a?}
    H -->|SÃ­| I[Cargar siguiente directorio]
    I --> G
    H -->|No| F
```

**Ventajas:**
- Discovery se ejecuta solo cuando el job arranca (no al iniciar la app)
- Memoria eficiente: O(D) donde D = archivos en directorio actual
- Una sola sesiÃ³n SFTP para todo el escaneo
- Procesamiento BFS (Breadth-First Search) recursivo

**ImplementaciÃ³n:**
```java
public SftpFileEntry read() {
    // Primera llamada: discovery completo
    if (!discoveryCompleted) {
        executeDirectoryDiscovery();  // BFS recursivo
        discoveryCompleted = true;
    }
    
    // Retornar archivos del directorio actual
    if (!currentDirectoryFiles.isEmpty()) {
        return currentDirectoryFiles.poll();
    }
    
    // Si no hay mÃ¡s directorios, terminar
    if (directoryQueue.isEmpty()) {
        return null;
    }
    
    // Cargar siguiente directorio
    loadDirectoryFiles(directoryQueue.poll());
    return read();
}
```

#### 2. **Async Processing Pipeline**

```mermaid
flowchart LR
    R[Reader<br/>1 thread] --> Q1[Queue<br/>Capacity: 1000]
    Q1 --> P1[Processor<br/>Thread 1]
    Q1 --> P2[Processor<br/>Thread 2]
    Q1 --> P3[Processor<br/>...]
    Q1 --> PN[Processor<br/>Thread N]
    
    P1 --> Q2[Queue]
    P2 --> Q2
    P3 --> Q2
    PN --> Q2
    
    Q2 --> W[Writer<br/>Bulk Operations]
```

**Ventajas:**
- Paralelismo configurable (20 threads por defecto)
- No bloqueante: Reader continÃºa mientras se procesa
- Processor sin I/O: Solo transformaciÃ³n en memoria
- Queue capacity evita saturaciÃ³n de memoria

**ConfiguraciÃ³n:**
```java
@Bean
AsyncItemProcessor<SftpFileEntry, ArchivoMetadata> asyncMetadataProcessor() {
    AsyncItemProcessor<SftpFileEntry, ArchivoMetadata> asyncProcessor = 
        new AsyncItemProcessor<>(metadataExtractorProcessor);
    asyncProcessor.setTaskExecutor(indexingTaskExecutor());
    return asyncProcessor;
}

@Bean
TaskExecutor indexingTaskExecutor() {
    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
    executor.setCorePoolSize(batchProps.getThreadPoolSize());  // 20
    executor.setMaxPoolSize(batchProps.getThreadPoolSize());
    executor.setQueueCapacity(batchProps.getQueueCapacity());  // 1000
    executor.initialize();
    return executor;
}
```

#### 3. **Bulk Upsert Strategy**

```mermaid
sequenceDiagram
    participant Writer
    participant BulkOps
    participant MongoDB
    
    Writer->>BulkOps: Crear operaciÃ³n UNORDERED
    
    loop Por cada ArchivoMetadata
        Writer->>BulkOps: upsert(query, update)
    end
    
    Writer->>BulkOps: execute()
    BulkOps->>MongoDB: Bulk write (1 round-trip)
    MongoDB-->>BulkOps: BulkWriteResult
    BulkOps-->>Writer: inserted + modified counts
```

**Performance:**
- Sin bulk: 100-200 docs/s (1 round-trip por doc)
- Con bulk: 3000-5000 docs/s (1 round-trip por chunk)
- Para 11M archivos: ~30-60 minutos vs ~15-30 horas

**ImplementaciÃ³n:**
```java
public void write(Chunk<? extends ArchivoMetadata> chunk) {
    BulkOperations bulkOps = mongoTemplate.bulkOps(
        BulkOperations.BulkMode.UNORDERED,
        DisorganizedFilesIndexDocument.class
    );
    
    for (ArchivoMetadata metadata : chunk) {
        Query query = new Query(Criteria.where("idUnico").is(metadata.getIdUnico()));
        Update update = /* build update */;
        bulkOps.upsert(query, update);
    }
    
    BulkWriteResult result = bulkOps.execute();
}
```

#### 4. **Connection Pool Lifecycle**

```mermaid
stateDiagram-v2
    [*] --> Idle: Pool inicializado (lazy)
    
    Idle --> Creating: getSession() llamado
    Creating --> Active: SesiÃ³n creada
    Active --> Validating: testOnBorrow=true
    Validating --> InUse: ValidaciÃ³n OK
    Validating --> Destroyed: ValidaciÃ³n falla
    
    InUse --> Returning: close() llamado
    Returning --> Idle: Devuelta al pool
    
    Idle --> Evicting: Eviction timer
    Evicting --> Destroyed: Idle > minEvictableIdleTime
    
    Destroyed --> [*]
```

**Ventajas:**
- Lazy init: No conexiones al inicio
- testOnBorrow: Detecta conexiones zombie
- Eviction: Libera recursos automÃ¡ticamente
- ValidaciÃ³n pre-uso con comando SFTP simple

**ConfiguraciÃ³n:**
```properties
sftp.origin.pool.lazy-init=true
sftp.origin.pool.initial-size=0
sftp.origin.pool.max-size=10
sftp.origin.pool.test-on-borrow=true
sftp.origin.pool.time-between-eviction-runs-millis=60000
sftp.origin.pool.min-evictable-idle-time-millis=300000
```

### Sistema de AuditorÃ­a

#### Arquitectura de AuditorÃ­a

```mermaid
graph TB
    subgraph "Spring Batch Metadata (PostgreSQL)"
        PG_JOB[BATCH_JOB_EXECUTION<br/>Estado global]
        PG_STEP[BATCH_STEP_EXECUTION<br/>MÃ©tricas tÃ©cnicas]
        PG_JOB --> PG_STEP
    end
    
    subgraph "AuditorÃ­a de Negocio (MongoDB)"
        MONGO_AUDIT[job_executions_audit<br/>MÃ©tricas + KPIs]
    end
    
    subgraph "JobExecutionListener"
        LISTENER[beforeJob/afterJob]
    end
    
    LISTENER -->|Lee mÃ©tricas| PG_STEP
    LISTENER -->|Calcula KPIs| MONGO_AUDIT
    
    style PG_JOB fill:#87CEEB
    style PG_STEP fill:#87CEEB
    style MONGO_AUDIT fill:#90EE90
```

#### Datos Capturados en AuditorÃ­a

**InformaciÃ³n bÃ¡sica:**
- `auditId`: ID Ãºnico (jobName-jobExecutionId-uuid)
- `jobExecutionId`: Referencia a Spring Batch
- `serviceName`: Nombre del microservicio
- `jobName`: Nombre del job (BATCH-INDEX-FULL)

**Tiempos y duraciÃ³n:**
- `startTime`: Fecha/hora de inicio
- `endTime`: Fecha/hora de fin
- `durationMs`: DuraciÃ³n en milisegundos
- `durationFormatted`: Formato legible (ej: "29m 55s")

**Estado y resultados:**
- `status`: STARTED, COMPLETED, FAILED, STOPPED
- `exitCode`: COMPLETED, FAILED, UNKNOWN
- `exitDescription`: DescripciÃ³n del resultado

**MÃ©tricas de procesamiento:**
- `totalFilesIndexed`: Total de archivos indexados (write_count)
- `totalFilesProcessed`: Total procesados (read_count)
- `totalFilesSkipped`: Total saltados (skips + filters)
- `totalFilesFailed`: Total fallidos (write_skip + rollbacks)
- `totalDirectoriesProcessed`: Directorios escaneados

**MÃ©tricas de rendimiento:**
- `readCount`, `writeCount`, `commitCount`, `rollbackCount`
- `filesPerSecond`: Throughput calculado (â­ Ãºnico en auditorÃ­a)

**InformaciÃ³n de errores:**
- `errorDescription`: Mensaje del error principal
- `errorStackTrace`: Stack trace truncado (10 lÃ­neas)
- `failureCount`: NÃºmero de fallos

**InformaciÃ³n del servidor:**
- `hostname`: Host donde se ejecutÃ³
- `instanceId`: Pod de Kubernetes (si aplica)
- `jobParameters`: ParÃ¡metros de entrada

#### ComparaciÃ³n: Spring Batch vs AuditorÃ­a

| Aspecto | Spring Batch (PostgreSQL) | AuditorÃ­a (MongoDB) |
|---------|--------------------------|---------------------|
| **PropÃ³sito** | Control tÃ©cnico del framework | MÃ©tricas de negocio y BI |
| **Usuario** | Spring Batch internamente | Dashboards, reportes, analistas |
| **Queries** | JOINs complejos | Queries simples |
| **Performance lectura** | Lento (mÃºltiples tablas) | RÃ¡pido (documento Ãºnico) |
| **Datos Ãºnicos** | ExecutionContext, versioning | filesPerSecond, hostname, KPIs |
| **Uso** | Restart jobs, recovery | Reportes, anÃ¡lisis, alertas |

**Ejemplo de redundancia beneficiosa:**

```javascript
// âœ… Query simple en MongoDB (auditorÃ­a)
db.job_executions_audit.find({
  jobName: "BATCH-INDEX-FULL",
  status: "COMPLETED",
  filesPerSecond: { $gt: 5000 }
}).sort({ startTime: -1 })

// vs

// âŒ Query compleja en PostgreSQL (Spring Batch)
SELECT 
    je.job_execution_id,
    je.status,
    se.write_count,
    (se.write_count::float / EXTRACT(EPOCH FROM (je.end_time - je.start_time))) as files_per_second
FROM batch_job_execution je
JOIN batch_step_execution se ON je.job_execution_id = se.job_execution_id
JOIN batch_job_instance ji ON je.job_instance_id = ji.job_instance_id
WHERE ji.job_name = 'BATCH-INDEX-FULL'
  AND je.status = 'COMPLETED'
  AND (se.write_count::float / EXTRACT(EPOCH FROM (je.end_time - je.start_time))) > 5000
ORDER BY je.start_time DESC;
```

---

## ğŸ“¦ Requisitos Previos

### Software Requerido

- **JDK 21** (OpenJDK o Oracle)
- **Maven 3.8+**
- **MongoDB 5.0+**
- **PostgreSQL 12+**
- **Servidor SFTP** con acceso configurado

### Recursos MÃ­nimos

**Desarrollo:**
- RAM: 4 GB
- CPU: 2 cores
- Disco: 2 GB

**ProducciÃ³n (11M archivos):**
- RAM: 8-16 GB (segÃºn `batch.thread-pool-size`)
- CPU: 4-8 cores
- Disco: 10 GB (logs + metadata temporal de Batch)

---

## ğŸš€ InstalaciÃ³n y Setup

### CompilaciÃ³n del Proyecto

```bash
# Clonar repositorio
git clone <repository-url>
cd dvsmart_indexing_api

# Verificar Maven
mvn -version

# Limpiar y compilar (skip tests)
mvn clean package -DskipTests

# Compilar y ejecutar tests
mvn clean install
```

**Artefacto generado:** `target/dvsmart_indexing_api.jar`

### Aplicar Licencias (CopyRight Headers)

```bash
# Aplicar headers a todos los archivos .java
mvn license:format

# Verificar headers
mvn license:check
```

### Inicializar Base de Datos MongoDB

**âš ï¸ IMPORTANTE:** Ejecutar el script de inicializaciÃ³n ANTES del primer despliegue.

```bash
# Desarrollo (local con NodePort)
mongo localhost:30000/dvsmart-ms \
  -u dvsmart_user \
  -p eoQQqfTyMd \
  --authenticationDatabase dvsmart-ms \
  scripts/mongodb/01_init_collections.js

# ProducciÃ³n (cluster interno)
mongo dvsmart-catalog-mongodb.dvsmart.svc.cluster.local:27017/dvsmart-ms \
  -u dvsmart_user \
  -p eoQQqfTyMd \
  --authenticationDatabase dvsmart-ms \
  scripts/mongodb/01_init_collections.js
```

**El script crea:**
- âœ… ColecciÃ³n `files_index` con validaciÃ³n de esquema y 7 Ã­ndices
- âœ… ColecciÃ³n `job_executions_audit` con validaciÃ³n de esquema y 9 Ã­ndices
- âœ… Documentos de ejemplo en ambas colecciones
- âœ… Usuario de aplicaciÃ³n con permisos readWrite

**Verificar que todo estÃ¡ OK:**

```javascript
// Conectar a MongoDB
mongo localhost:30000/dvsmart-ms -u dvsmart_user -p eoQQqfTyMd --authenticationDatabase dvsmart-ms

// Verificar colecciones
show collections
// Debe mostrar: files_index, job_executions_audit

// Verificar Ã­ndices de files_index
db.files_index.getIndexes().length
// Debe retornar: 8 (incluyendo _id)

// Verificar Ã­ndices de job_executions_audit
db.job_executions_audit.getIndexes().length
// Debe retornar: 10 (incluyendo _id)

// Verificar documentos de ejemplo
db.files_index.countDocuments()
// Debe retornar: 5

db.job_executions_audit.countDocuments()
// Debe retornar: 3
```

### ConfiguraciÃ³n de PostgreSQL

**Crear usuario y base de datos:**

```sql
-- Conectar como superusuario
psql -U postgres

-- Crear base de datos
CREATE DATABASE dvsmart;

-- Crear usuario
CREATE USER dvsmart_ms WITH PASSWORD 'OgxjdNEeQl';

-- Otorgar permisos
GRANT ALL PRIVILEGES ON DATABASE dvsmart TO dvsmart_ms;

-- Conectar a la base de datos
\c dvsmart

-- Otorgar permisos en el schema
GRANT ALL ON SCHEMA public TO dvsmart_ms;
```

**Verificar tablas de Spring Batch:**

DespuÃ©s del primer arranque, Spring Batch crearÃ¡ automÃ¡ticamente las siguientes tablas:

```sql
-- Verificar tablas creadas
\dt

-- Debe mostrar:
-- BATCH_JOB_INSTANCE
-- BATCH_JOB_EXECUTION
-- BATCH_JOB_EXECUTION_PARAMS
-- BATCH_STEP_EXECUTION
-- BATCH_JOB_EXECUTION_CONTEXT
-- BATCH_STEP_EXECUTION_CONTEXT
```

### ConfiguraciÃ³n del Servidor SFTP

**Verificar conectividad:**

```bash
# ConexiÃ³n manual
sftp -P 22 sftpsourceuser@sftp-host

# Test de latencia
ping sftp-host

# Test de puerto
telnet sftp-host 22
```

### EjecuciÃ³n Local

```bash
# Modo development
mvn spring-boot:run

# O usando el JAR compilado
java -jar target/dvsmart_indexing_api.jar

# Con perfil prod
java -jar target/dvsmart_indexing_api.jar --spring.profiles.active=prod

# Sobreescribir propiedades
java -jar target/dvsmart_indexing_api.jar \
  --server.port=9090 \
  --spring.mongodb.uri=mongodb://localhost:27017/test
```

La aplicaciÃ³n estarÃ¡ disponible en:
```
http://localhost:8080/dvsmart_indexing_api
```

---

## âš™ï¸ GuÃ­a Completa de ConfiguraciÃ³n

### Archivo: `application.properties`

#### ConfiguraciÃ³n Base

```properties
# AplicaciÃ³n
spring.application.name=dvsmart-indexing-api
server.servlet.context-path=/dvsmart_indexing_api
server.port=8080
server.shutdown=graceful
```

| Propiedad | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `spring.application.name` | `dvsmart-indexing-api` | Nombre usado en auditorÃ­a |
| `server.servlet.context-path` | `/dvsmart_indexing_api` | Context path base |
| `server.port` | `8080` | Puerto HTTP |
| `server.shutdown` | `graceful` | Espera jobs activos antes de cerrar |

#### MongoDB

```properties
spring.mongodb.uri=mongodb://dvsmart_user:eoQQqfTyMd@localhost:30000/dvsmart-ms?authSource=dvsmart-ms

# âœ… CRÃTICO: Desactivar auto-creaciÃ³n de Ã­ndices
# Los Ã­ndices se crean mediante script 01_init_collections.js
spring.data.mongodb.auto-index-creation=false
```

**Colecciones utilizadas:**

| ColecciÃ³n | PropÃ³sito | Ãndices | Documentos ejemplo |
|-----------|-----------|---------|-------------------|
| `files_index` | Metadata de archivos indexados | 8 Ã­ndices (1 Ãºnico, 7 bÃºsqueda) | 5 |
| `job_executions_audit` | AuditorÃ­a de ejecuciones | 10 Ã­ndices (2 Ãºnicos, 8 bÃºsqueda) | 3 |

#### PostgreSQL (Spring Batch Repository)

```properties
spring.datasource.url=jdbc:postgresql://localhost:30005/dvsmart
spring.datasource.driver-class-name=org.postgresql.Driver
spring.datasource.username=dvsmart_ms
spring.datasource.password=OgxjdNEeQl
spring.datasource.hikari.maximum-pool-size=10
spring.datasource.hikari.minimum-idle=5
```

**Tablas de Spring Batch (creadas automÃ¡ticamente):**

| Tabla | PropÃ³sito |
|-------|-----------|
| `BATCH_JOB_INSTANCE` | Instancias de jobs |
| `BATCH_JOB_EXECUTION` | Ejecuciones de jobs |
| `BATCH_STEP_EXECUTION` | Ejecuciones de steps (â­ mÃ©tricas detalladas) |
| `BATCH_JOB_EXECUTION_PARAMS` | ParÃ¡metros de ejecuciÃ³n |

#### Spring Batch

```properties
spring.batch.job.enabled=false
spring.batch.jdbc.initialize-schema=always
```

| Propiedad | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `spring.batch.job.enabled` | `false` | âš ï¸ **CRÃTICO:** Desactiva inicio automÃ¡tico |
| `spring.batch.jdbc.initialize-schema` | `always` | Crea tablas al inicio |

#### ConfiguraciÃ³n del Batch (Prefijo: `batch.*`)

```properties
batch.chunk-size=100
batch.thread-pool-size=20
batch.queue-capacity=1000
batch.skip-limit=5
batch.retry-limit=3
```

| Propiedad | Valor | Rango | DescripciÃ³n |
|-----------|-------|-------|-------------|
| `batch.chunk-size` | `100` | `50-1000` | Archivos procesados por chunk antes de commit |
| `batch.thread-pool-size` | `20` | `10-50` | Threads del `AsyncItemProcessor/Writer` |
| `batch.queue-capacity` | `1000` | `500-5000` | Capacidad de cola de tareas pendientes |
| `batch.skip-limit` | `5` | `0-100` | MÃ¡ximo de errores saltables por chunk |
| `batch.retry-limit` | `3` | `0-10` | Reintentos antes de fallar |

**Configuraciones por entorno:**

```properties
# ğŸ”¹ DESARROLLO
batch.chunk-size=50
batch.thread-pool-size=5
batch.queue-capacity=500

# ğŸ”¹ PRODUCCIÃ“N ESTÃNDAR
batch.chunk-size=100
batch.thread-pool-size=20
batch.queue-capacity=1000

# ğŸ”¹ ALTO RENDIMIENTO (11M archivos)
batch.chunk-size=500
batch.thread-pool-size=50
batch.queue-capacity=5000
```

#### Servidor SFTP Origen (Prefijo: `sftp.origin.*`)

```properties
sftp.origin.host=localhost
sftp.origin.port=30002
sftp.origin.user=sftpsourceuser
sftp.origin.password=securepass
sftp.origin.base-dir=/disorganized_data
sftp.origin.timeout=30000
```

#### Pool de Conexiones SFTP (Prefijo: `sftp.origin.pool.*`)

```properties
# Pool lazy
sftp.origin.pool.lazy-init=true
sftp.origin.pool.initial-size=0
sftp.origin.pool.max-size=10
sftp.origin.pool.size=10

# Timeouts y validaciÃ³n
sftp.origin.pool.max-wait-millis=30000
sftp.origin.pool.test-on-borrow=true
sftp.origin.pool.test-while-idle=true

# Eviction (limpieza de idle)
sftp.origin.pool.time-between-eviction-runs-millis=60000
sftp.origin.pool.min-evictable-idle-time-millis=300000
```

| Propiedad | Valor | DescripciÃ³n |
|-----------|-------|-------------|
| `lazy-init` | `true` | No crear conexiones al inicio |
| `initial-size` | `0` | Pool completamente lazy |
| `max-size` | `10` | TamaÃ±o mÃ¡ximo del pool |
| `test-on-borrow` | `true` | **CRÃTICO:** Validar antes de usar |
| `min-evictable-idle-time-millis` | `300000` | 5 min idle antes de cerrar |

#### Logging

```properties
# Niveles generales
logging.level.root=INFO
logging.level.com.indra.minsait.dvsmart.indexing=DEBUG

# Componentes especÃ­ficos
logging.level.org.springframework.batch=INFO
logging.level.org.springframework.integration.sftp=DEBUG
logging.level.org.springframework.data.mongodb=INFO

# PatrÃ³n
logging.pattern.console=%d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %msg%n
```

#### Actuator (MonitorizaciÃ³n)

```properties
management.endpoints.web.exposure.include=health,info,metrics,batch
management.endpoint.health.show-details=always

# MÃ©tricas
management.metrics.enable.jvm=true
management.metrics.enable.process=true
management.metrics.enable.system=true
```

---

## ğŸ”¥ ConfiguraciÃ³n de Alto Rendimiento

### Tuning de la JVM

```bash
# Variables de entorno
export JAVA_OPTS="-Xms4g -Xmx8g -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=4"

java $JAVA_OPTS -jar target/dvsmart_indexing_api.jar
```

| Flag | Valor | PropÃ³sito |
|------|-------|-----------|
| `-Xms4g` | Heap inicial 4GB | Evita resizing |
| `-Xmx8g` | Heap mÃ¡ximo 8GB | Suficiente para 11M archivos |
| `-XX:+UseG1GC` | G1 GC | Baja latencia |
| `-XX:MaxGCPauseMillis=200` | Pausas < 200ms | Reduce impacto del GC |

### Tuning de Spring Batch (11M archivos)

```properties
batch.chunk-size=500
batch.thread-pool-size=50
batch.queue-capacity=5000
sftp.origin.pool.max-size=10
```

**CÃ¡lculo de throughput:**

```
Archivos: 11,000,000
Chunk size: 500
Threads: 50
Tiempo por chunk: 2s

Chunks totales: 11,000,000 / 500 = 22,000
Tiempo: (22,000 / 50) * 2s = 880s â‰ˆ 15 minutos
```

### Tuning de MongoDB

**Ãndices ya creados por script:**

```javascript
// âœ… Ya estÃ¡n en 01_init_collections.js
// files_index: 8 Ã­ndices
// job_executions_audit: 10 Ã­ndices
```

**Write concern para alto throughput:**

```properties
# Agregar a URI
spring.mongodb.uri=mongodb://user:pass@host:27017/db?w=1&maxPoolSize=100
```

### Tuning de PostgreSQL

```sql
-- Aumentar shared_buffers para cache
ALTER SYSTEM SET shared_buffers = '2GB';

-- Aumentar work_mem para ordenamientos
ALTER SYSTEM SET work_mem = '64MB';

-- Checkpoint menos frecuentes
ALTER SYSTEM SET checkpoint_timeout = '30min';

-- Aplicar cambios
SELECT pg_reload_conf();
```

---

## ğŸ“¡ Uso y API

### Endpoints de IndexaciÃ³n

#### ğŸ”µ Iniciar IndexaciÃ³n Completa

```http
POST /dvsmart_indexing_api/api/batch/index/full
Content-Type: application/json

{
  "jobName": "BATCH-INDEX-FULL",
  "parameters": {}
}
```

**Response (202 Accepted):**

```json
{
  "message": "Batch job started successfully",
  "jobExecutionId": 12345,
  "status": "ACCEPTED"
}
```

**Ejemplos:**

```bash
# Local
curl -X POST http://localhost:8080/dvsmart_indexing_api/api/batch/index/full \
  -H "Content-Type: application/json" \
  -d '{"jobName":"BATCH-INDEX-FULL","parameters":{}}'
```

### Endpoints de AuditorÃ­a

#### ğŸ“Š Historial de AuditorÃ­a de un Job

```http
GET /dvsmart_indexing_api/api/monitoring/audit/jobs/{jobName}
```

**Ejemplo:**

```bash
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/jobs/BATCH-INDEX-FULL | jq
```

**Response:**

```json
[
  {
    "id": "676b1234567890abcdef1234",
    "auditId": "BATCH-INDEX-FULL-12345-a1b2c3d4",
    "jobExecutionId": 12345,
    "serviceName": "dvsmart-indexing-api",
    "jobName": "BATCH-INDEX-FULL",
    "startTime": "2025-12-26T10:00:00Z",
    "endTime": "2025-12-26T10:30:00Z",
    "durationMs": 1800000,
    "durationFormatted": "30m 0s",
    "status": "COMPLETED",
    "exitCode": "COMPLETED",
    "totalFilesIndexed": 11000000,
    "totalFilesProcessed": 11050000,
    "totalFilesSkipped": 50000,
    "totalFilesFailed": 0,
    "filesPerSecond": 6111.11,
    "hostname": "indexing-api-pod-abc123",
    "instanceId": "indexing-api-pod-abc123"
  }
]
```

#### ğŸ“Š Ejecuciones por Estado

```http
GET /dvsmart_indexing_api/api/monitoring/audit/status/{status}
```

**Ejemplos:**

```bash
# Ejecuciones completadas
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/status/COMPLETED | jq

# Ejecuciones fallidas
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/status/FAILED | jq

# Ejecuciones en curso
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/status/STARTED | jq
```

#### ğŸ“Š Detalle de una EjecuciÃ³n

```http
GET /dvsmart_indexing_api/api/monitoring/audit/execution/{jobExecutionId}
```

**Ejemplo:**

```bash
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/execution/12345 | jq
```

#### ğŸ“Š EstadÃ­sticas Globales de AuditorÃ­a

```http
GET /dvsmart_indexing_api/api/monitoring/audit/stats
```

**Response:**

```json
{
  "totalExecutions": 150,
  "completedExecutions": 140,
  "failedExecutions": 8,
  "startedExecutions": 2
}
```

#### ğŸ“Š Ãšltimas Ejecuciones

```http
GET /dvsmart_indexing_api/api/monitoring/audit/latest
```

#### ğŸ“Š Ejecuciones en Rango de Fechas

```http
GET /dvsmart_indexing_api/api/monitoring/audit/range?start={ISO_DATE}&end={ISO_DATE}
```

**Ejemplo:**

```bash
curl "http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/range?start=2025-12-01T00:00:00Z&end=2025-12-31T23:59:59Z" | jq
```

### Endpoints de MonitorizaciÃ³n SFTP Pool

#### ğŸŸ¢ EstadÃ­sticas BÃ¡sicas del Pool

```http
GET /dvsmart_indexing_api/api/monitoring/sftp-pool
```

**Response:**

```json
{
  "active": 2,
  "idle": 3,
  "maxTotal": 10,
  "totalCreated": 5,
  "totalDestroyed": 0,
  "utilizationPercent": 20.0,
  "availableSlots": 8
}
```

#### ğŸŸ¢ Health Check del Pool

```http
GET /dvsmart_indexing_api/api/monitoring/sftp-pool/health
```

**Estados:**

| Status | CondiciÃ³n | AcciÃ³n |
|--------|-----------|--------|
| `HEALTHY` | utilization < 80% && failures < 10% | Normal |
| `WARNING` | utilization 80-95% | Aumentar pool |
| `DEGRADED` | failures > 10% | Revisar SFTP |
| `CRITICAL` | utilization > 95% | Urgente |

### Endpoints de Batch Jobs

#### ğŸ” Detalle de una EjecuciÃ³n (Spring Batch)

```http
GET /dvsmart_indexing_api/api/monitoring/jobs/execution/{executionId}
```

**Response incluye:**
- Estado del job
- MÃ©tricas de todos los steps
- ParÃ¡metros de entrada
- Tiempos de ejecuciÃ³n

---

## ğŸ“Š MonitorizaciÃ³n y Observabilidad

### Health Checks

```bash
# Health general
curl http://localhost:8080/dvsmart_indexing_api/actuator/health | jq

# Debe mostrar:
# - MongoDB: UP
# - PostgreSQL: UP
# - diskSpace: UP
```

### MÃ©tricas Disponibles

| Fuente | MÃ©tricas | PropÃ³sito |
|--------|----------|-----------|
| **PostgreSQL** | read_count, write_count, commit_count | Control tÃ©cnico de Spring Batch |
| **MongoDB (auditorÃ­a)** | filesPerSecond, throughput, KPIs | Dashboards y reportes de negocio |
| **Pool SFTP** | active, idle, utilization | Monitoreo de recursos |

### Logs Estructurados

**Ejemplo de log completo:**

```
2025-12-26 10:00:00 - Starting FULL INDEXING JOB
2025-12-26 10:00:00 - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-26 10:00:00 - ğŸ“‹ AUDIT: Creating audit record for job execution
2025-12-26 10:00:00 -    Job Name: BATCH-INDEX-FULL
2025-12-26 10:00:00 -    Execution ID: 12345
2025-12-26 10:00:00 - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-26 10:00:00 - âœ… Audit record created: auditId=BATCH-INDEX-FULL-12345-a1b2c3d4, jobExecutionId=12345
2025-12-26 10:00:01 - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-26 10:00:01 - ğŸ”„ OPEN: Initializing DirectoryQueueItemReader
2025-12-26 10:00:01 - Base directory: /disorganized_data
2025-12-26 10:00:01 - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-26 10:00:02 - PHASE 1: DIRECTORY DISCOVERY
2025-12-26 10:00:25 - âœ… Discovery completed in 23000 ms (23 seconds)
2025-12-26 10:00:25 - Total directories to process: 8543
2025-12-26 10:00:25 - PHASE 2: FILE INDEXING
2025-12-26 10:00:32 - Bulk write completed: 100 inserted, 0 updated | Success: 100, Failed: 0
2025-12-26 10:05:00 - ğŸ“Š Progress: 100 directories processed, 12500 files indexed
2025-12-26 10:29:30 - âœ… INDEXING COMPLETED
2025-12-26 10:29:30 - Total files indexed: 11000000
2025-12-26 10:29:30 - Total directories processed: 8543
2025-12-26 10:29:30 - â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
2025-12-26 10:29:30 - ğŸ“‹ AUDIT: Updating audit record for job execution
2025-12-26 10:29:30 -    Status: COMPLETED
2025-12-26 10:29:30 - âœ… Audit record updated: auditId=BATCH-INDEX-FULL-12345-a1b2c3d4, status=COMPLETED, filesIndexed=11000000, duration=30m 0s
```

### Consultas Ãštiles en MongoDB

```javascript
// Top 10 ejecuciones mÃ¡s rÃ¡pidas
db.job_executions_audit.find({
  status: "COMPLETED"
}).sort({ filesPerSecond: -1 }).limit(10)

// Ejecuciones con mÃ¡s de 10M archivos
db.job_executions_audit.find({
  totalFilesIndexed: { $gt: 10000000 }
})

// Promedio de throughput
db.job_executions_audit.aggregate([
  { $match: { status: "COMPLETED" } },
  { $group: {
      _id: null,
      avgThroughput: { $avg: "$filesPerSecond" },
      maxThroughput: { $max: "$filesPerSecond" },
      minThroughput: { $min: "$filesPerSecond" }
  }}
])

// Ejecuciones por dÃ­a (Ãºltimos 30 dÃ­as)
db.job_executions_audit.aggregate([
  {
    $match: {
      startTime: {
        $gte: new Date(new Date().setDate(new Date().getDate() - 30))
      }
    }
  },
  {
    $group: {
      _id: { $dateToString: { format: "%Y-%m-%d", date: "$startTime" } },
      count: { $sum: 1 },
      avgDuration: { $avg: "$durationMs" },
      totalFiles: { $sum: "$totalFilesIndexed" }
    }
  },
  { $sort: { _id: 1 } }
])
```

---

## ğŸ”§ Troubleshooting

### Errores Comunes

| Error | Causa | SoluciÃ³n |
|-------|-------|----------|
| `Could not obtain SFTP session` | Pool saturado | Aumentar `pool.max-size` |
| `Connection reset by peer` | Servidor cerrÃ³ idle | Reducir `min-evictable-idle-time-millis` |
| `Auth fail` | Credenciales incorrectas | Verificar `user` y `password` |
| `OutOfMemoryError` | Heap insuficiente | Aumentar `-Xmx` |
| `E11000 duplicate key` | Ãndice violado | `db.collection.reIndex()` |
| `Audit record not found` | Job no registrado | Verificar listener configurado |

### Comandos de DiagnÃ³stico

```bash
# Verificar conectividad SFTP
telnet sftp-host 22

# Verificar MongoDB
mongo localhost:30000/dvsmart-ms -u dvsmart_user -p eoQQqfTyMd --authenticationDatabase dvsmart-ms

# Verificar PostgreSQL
psql -h localhost -p 30005 -U dvsmart_ms -d dvsmart

# Verificar jobs en ejecuciÃ³n
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/jobs/running | jq

# Verificar Ãºltima auditorÃ­a
curl http://localhost:8080/dvsmart_indexing_api/api/monitoring/audit/latest | jq
```

### DiagnÃ³stico de AuditorÃ­a

```javascript
// Conectar a MongoDB
mongo localhost:30000/dvsmart-ms -u dvsmart_user -p eoQQqfTyMd --authenticationDatabase dvsmart-ms

// Verificar registros de auditorÃ­a
db.job_executions_audit.find().sort({ startTime: -1 }).limit(5).pretty()

// Verificar ejecuciones sin finalizar
db.job_executions_audit.find({
  status: "STARTED",
  startTime: { $lt: new Date(Date.now() - 3600000) }  // MÃ¡s de 1 hora
})

// Verificar ejecuciones fallidas recientes
db.job_executions_audit.find({
  status: "FAILED",
  startTime: { $gte: new Date(Date.now() - 86400000) }  // Ãšltimas 24h
}).sort({ startTime: -1 })
```

---

## ğŸ§ª Mantenimiento y Testing

### Tests Unitarios

```bash
# Ejecutar todos los tests
mvn test

# Ejecutar un test especÃ­fico
mvn test -Dtest=JobAuditServiceTest

# Tests con cobertura
mvn clean test jacoco:report
```

### Limpieza de Datos

**MongoDB:**

```javascript
// Eliminar auditorÃ­as antiguas (mÃ¡s de 90 dÃ­as)
db.job_executions_audit.deleteMany({
  createdAt: { $lt: new Date(Date.now() - 90*24*60*60*1000) }
})

// Eliminar solo ejecuciones fallidas antiguas
db.job_executions_audit.deleteMany({
  status: "FAILED",
  createdAt: { $lt: new Date(Date.now() - 30*24*60*60*1000) }
})

// Verificar espacio usado
db.job_executions_audit.stats()
```

**PostgreSQL:**

```sql
-- Limpiar jobs antiguos (mÃ¡s de 30 dÃ­as)
DELETE FROM BATCH_STEP_EXECUTION
WHERE job_execution_id IN (
  SELECT job_execution_id
  FROM BATCH_JOB_EXECUTION
  WHERE create_time < NOW() - INTERVAL '30 days'
);

DELETE FROM BATCH_JOB_EXECUTION_PARAMS
WHERE job_execution_id IN (
  SELECT job_execution_id
  FROM BATCH_JOB_EXECUTION
  WHERE create_time < NOW() - INTERVAL '30 days'
);

DELETE FROM BATCH_JOB_EXECUTION
WHERE create_time < NOW() - INTERVAL '30 days';

-- Vacuum
VACUUM FULL;
```

### ReinicializaciÃ³n Completa

```bash
# 1. Detener la aplicaciÃ³n
kill {pid}

# 2. Limpiar MongoDB
mongo localhost:30000/dvsmart-ms -u dvsmart_user -p eoQQqfTyMd --authenticationDatabase dvsmart-ms <<EOF
use dvsmart-ms
db.files_index.deleteMany({})
db.job_executions_audit.deleteMany({})
EOF

# 3. Limpiar PostgreSQL
psql -h localhost -p 30005 -U dvsmart_ms -d dvsmart <<EOF
TRUNCATE TABLE BATCH_STEP_EXECUTION CASCADE;
TRUNCATE TABLE BATCH_JOB_EXECUTION_PARAMS CASCADE;
TRUNCATE TABLE BATCH_JOB_EXECUTION CASCADE;
TRUNCATE TABLE BATCH_JOB_INSTANCE CASCADE;
EOF

# 4. Reiniciar
java -jar target/dvsmart_indexing_api.jar
```

---

## ğŸ“š Referencias

- [Spring Batch Documentation](https://docs.spring.io/spring-batch/docs/current/reference/html/)
- [Spring Integration SFTP](https://docs.spring.io/spring-integration/docs/current/reference/html/sftp.html)
- [Apache Commons Pool2](https://commons.apache.org/proper/commons-pool/)
- [MongoDB Java Driver](https://www.mongodb.com/docs/drivers/java/sync/current/)
- [PostgreSQL JDBC](https://jdbc.postgresql.org/documentation/)

---

## ğŸ¤ Soporte y Contacto

**Equipo de Mantenimiento**: DvSmart Team  
**Contacto**: hahuaranga@indracompany.com  
**Repositorio**: [Enlace interno al repositorio]  
**DocumentaciÃ³n TÃ©cnica**: [Enlace a documentaciÃ³n detallada]